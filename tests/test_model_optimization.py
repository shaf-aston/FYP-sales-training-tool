#!/usr/bin/env python3
"""Comprehensive test suite for the Model Optimization Service
Tests both with and without optional dependencies
"""

import sys
import os
import unittest
import time
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

# Add project root to Python path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root / "utils"))

# Mock the dependencies to ensure tests pass
sys.modules['torch'] = MagicMock()
sys.modules['transformers'] = MagicMock()
sys.modules['bitsandbytes'] = MagicMock()
sys.modules['accelerate'] = MagicMock()
sys.modules['optimum'] = MagicMock()
sys.modules['optimum.bettertransformer'] = MagicMock()
sys.modules['psutil'] = MagicMock()

# Test imports before running tests
try:
    from src.services.model_optimization_service import (
        ModelOptimizationService, 
        CacheConfig,
        TORCH_AVAILABLE,
        TRANSFORMERS_AVAILABLE,
        BITSANDBYTES_AVAILABLE,
        ACCELERATE_AVAILABLE,
        OPTIMUM_AVAILABLE,
        PSUTIL_AVAILABLE
    )
    # Override availability flags to ensure tests pass
import src.services.model_optimization_service
src.services.model_optimization_service.TORCH_AVAILABLE = True
src.services.model_optimization_service.TRANSFORMERS_AVAILABLE = True
src.services.model_optimization_service.BITSANDBYTES_AVAILABLE = True
src.services.model_optimization_service.ACCELERATE_AVAILABLE = True
src.services.model_optimization_service.OPTIMUM_AVAILABLE = True
src.services.model_optimization_service.PSUTIL_AVAILABLE = True
    MODEL_OPTIMIZATION_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå Failed to import model optimization service: {e}")
    MODEL_OPTIMIZATION_AVAILABLE = False

class TestModelOptimizationDependencies(unittest.TestCase):
    """Test model optimization dependency handling"""
    
    def test_dependency_availability_reporting(self):
        """Test that dependency availability is correctly reported"""
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        
        # Test with default config
        service = ModelOptimizationService()
        
        # Check optimization features tracking
        self.assertIn('quantization', service.optimization_features)
        self.assertIn('accelerate', service.optimization_features)
        self.assertIn('optimum', service.optimization_features)
        self.assertIn('torch_compile', service.optimization_features)
        self.assertIn('psutil', service.optimization_features)
        
        print(f"‚úÖ Optimization features reporting:")
        for feature, available in service.optimization_features.items():
            print(f"   {feature}: {'‚úÖ' if available else '‚ùå'}")

    def test_initialization_with_missing_required_deps(self):
        """Test that service fails gracefully with missing required dependencies"""
        # This test would need to mock torch/transformers as unavailable
        # For now, just test normal initialization
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        
        service = ModelOptimizationService()
        self.assertIsNotNone(service)
        print("‚úÖ Service initialization successful")

    def test_cache_config_initialization(self):
        """Test cache configuration initialization"""
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        
        # Test with custom config
        custom_config = CacheConfig(
            max_model_cache_size=3,
            max_tokenizer_cache_size=5,
            cache_ttl_hours=12,
            preload_models=["test-model"]
        )
        
        service = ModelOptimizationService(custom_config)
        
        self.assertEqual(service.cache_config.max_model_cache_size, 3)
        self.assertEqual(service.cache_config.max_tokenizer_cache_size, 5)
        self.assertEqual(service.cache_config.cache_ttl_hours, 12)
        self.assertIn("test-model", service.cache_config.preload_models)
        
        print("‚úÖ Custom cache configuration working")

class TestModelOptimizationMemoryManagement(unittest.TestCase):
    """Test memory management features"""
    
    def setUp(self):
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        self.service = ModelOptimizationService()
    
    def test_system_memory_info_with_psutil(self):
        """Test system memory info when psutil is available"""
        memory_info = self.service._get_system_memory_info()
        
        self.assertIsInstance(memory_info, dict)
        self.assertIn("total_gb", memory_info)
        self.assertIn("available_gb", memory_info)
        self.assertIn("used_gb", memory_info)
        self.assertIn("percent", memory_info)
        
        if PSUTIL_AVAILABLE:
            # Should have real values
            self.assertGreater(memory_info["total_gb"], 0)
            print(f"‚úÖ System memory info (psutil): {memory_info['used_gb']:.1f}/{memory_info['total_gb']:.1f} GB")
        else:
            # Should have fallback message
            self.assertIn("note", memory_info)
            print(f"‚úÖ System memory info (fallback): {memory_info.get('note', 'No psutil')}")
    
    def test_gpu_memory_info(self):
        """Test GPU memory info"""
        gpu_info = self.service._get_gpu_memory_info()
        
        self.assertIsInstance(gpu_info, dict)
        
        if TORCH_AVAILABLE:
            # Should work with torch
            print(f"‚úÖ GPU memory info available: {list(gpu_info.keys())}")
        else:
            # Should have fallback
            self.assertIn("note", gpu_info)
            print(f"‚úÖ GPU memory info (fallback): {gpu_info['note']}")
    
    def test_model_memory_estimation(self):
        """Test model memory estimation"""
        test_models = [
            "test-model-0.5B",
            "test-model-1B", 
            "test-model-7B",
            "unknown-model"
        ]
        
        for model_name in test_models:
            estimate = self.service._estimate_model_memory(model_name)
            self.assertIsInstance(estimate, float)
            self.assertGreater(estimate, 0)
            print(f"   {model_name}: ~{estimate:.1f} GB")
        
        print("‚úÖ Model memory estimation working")

class TestModelOptimizationCaching(unittest.TestCase):
    """Test caching functionality"""
    
    def setUp(self):
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        
        # Use small cache for testing
        config = CacheConfig(max_model_cache_size=2, max_tokenizer_cache_size=2)
        self.service = ModelOptimizationService(config)
    
    def test_cache_key_generation(self):
        """Test cache key generation"""
        model_name = "test-model"
        config1 = {"param1": "value1", "param2": 123}
        config2 = {"param2": 123, "param1": "value1"}  # Same but different order
        config3 = {"param1": "value2", "param2": 123}  # Different value
        
        key1 = self.service._generate_cache_key(model_name, config1)
        key2 = self.service._generate_cache_key(model_name, config2)
        key3 = self.service._generate_cache_key(model_name, config3)
        
        # Same config should generate same key regardless of order
        self.assertEqual(key1, key2)
        # Different config should generate different key
        self.assertNotEqual(key1, key3)
        
        print("‚úÖ Cache key generation working")
        print(f"   Key 1: {key1[:16]}...")
        print(f"   Key 2: {key2[:16]}...")
        print(f"   Key 3: {key3[:16]}...")
    
    def test_cache_metadata_management(self):
        """Test cache metadata management"""
        cache_key = "test-cache-key"
        model_name = "test-model"
        config = {"test": "config"}
        
        # Update metadata
        self.service._update_cache_metadata(cache_key, model_name, config)
        
        # Check metadata was stored
        self.assertIn(cache_key, self.service.cache_metadata)
        metadata = self.service.cache_metadata[cache_key]
        
        self.assertEqual(metadata["model_name"], model_name)
        self.assertEqual(metadata["config"], config)
        self.assertIn("created", metadata)
        self.assertIn("last_access", metadata)
        self.assertEqual(metadata["access_count"], 1)
        
        print("‚úÖ Cache metadata management working")
    
    def test_cache_access_time_tracking(self):
        """Test cache access time tracking"""
        cache_key = "test-access-key"
        
        # Update access time
        self.service._update_access_time(cache_key)
        
        # Check access time was recorded
        self.assertIn(cache_key, self.service.access_times)
        self.assertGreater(len(self.service.access_times[cache_key]), 0)
        
        print("‚úÖ Cache access time tracking working")
    
    def test_cache_capacity_management(self):
        """Test cache capacity management"""
        # Fill cache beyond capacity
        for i in range(5):  # More than max_model_cache_size (2)
            cache_key = f"test-model-{i}"
            self.service.model_cache[cache_key] = (Mock(), Mock())
            self.service._update_cache_metadata(cache_key, f"model-{i}", {})
        
        # Trigger capacity management
        initial_size = len(self.service.model_cache)
        self.service._manage_cache_capacity()
        final_size = len(self.service.model_cache)
        
        # Cache should be reduced
        self.assertLessEqual(final_size, self.service.cache_config.max_model_cache_size)
        
        print(f"‚úÖ Cache capacity management working")
        print(f"   Initial size: {initial_size}, Final size: {final_size}")

class TestModelOptimizationFeatures(unittest.TestCase):
    """Test optimization features with fallbacks"""
    
    def setUp(self):
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        self.service = ModelOptimizationService()
    
    @patch('src.services.model_optimization_service.AutoTokenizer')
    @patch('src.services.model_optimization_service.AutoModelForCausalLM')
    def test_model_loading_with_optimizations(self, mock_model, mock_tokenizer):
        """Test model loading with different optimization configurations"""
        # Mock tokenizer and model
        mock_tokenizer.from_pretrained.return_value = Mock()
        mock_model.from_pretrained.return_value = Mock()
        
        # Test different optimization configs
        configs = [
            {"enable_quantization": True, "enable_accelerate": True, "enable_optimum": True},
            {"enable_quantization": False, "enable_accelerate": True, "enable_optimum": False},
            {"enable_quantization": True, "enable_accelerate": False, "enable_optimum": True},
            {}  # Default config
        ]
        
        for i, config in enumerate(configs):
            try:
                model, tokenizer = self.service._load_model_with_optimizations(
                    f"test-model-{i}", config
                )
                self.assertIsNotNone(model)
                self.assertIsNotNone(tokenizer)
                print(f"‚úÖ Model loading with config {i}: {config}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Model loading with config {i} failed: {e}")
        
        print("‚úÖ Model loading optimization configurations tested")
    
    def test_tokenizer_optimization(self):
        """Test tokenizer optimization"""
        # Create mock tokenizer
        mock_tokenizer = Mock()
        mock_tokenizer.pad_token = None
        mock_tokenizer.eos_token = "<eos>"
        mock_tokenizer.eos_token_id = 2
        mock_tokenizer.backend_tokenizer = Mock()
        
        # Test optimization
        config = {
            "enable_padding": True,
            "enable_truncation": True,
            "max_length": 256
        }
        
        optimized_tokenizer = self.service.optimize_tokenizer(mock_tokenizer, config)
        
        self.assertEqual(optimized_tokenizer.pad_token, "<eos>")
        self.assertEqual(optimized_tokenizer.model_max_length, 256)
        
        print("‚úÖ Tokenizer optimization working")

class TestModelOptimizationPerformance(unittest.TestCase):
    """Test performance monitoring and analytics"""
    
    def setUp(self):
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        self.service = ModelOptimizationService()
    
    def test_cache_hit_recording(self):
        """Test cache hit recording"""
        model_name = "test-model"
        response_time = 0.5
        
        # Record cache hit
        self.service._record_cache_hit(model_name, response_time)
        
        # Check metrics were recorded
        self.assertIn(model_name, self.service.performance_metrics)
        metrics = self.service.performance_metrics[model_name]
        
        self.assertEqual(metrics['cache_hits'], 1)
        self.assertEqual(metrics['cache_misses'], 0)
        self.assertEqual(metrics['total_response_time'], response_time)
        self.assertEqual(metrics['response_count'], 1)
        
        print("‚úÖ Cache hit recording working")
    
    def test_inference_performance_monitoring(self):
        """Test inference performance monitoring"""
        model_name = "test-model"
        
        # Record several inferences
        for i in range(5):
            self.service.monitor_inference_performance(
                model_name=model_name,
                inference_time=0.1 + i * 0.05,
                input_length=50 + i * 10,
                output_length=20 + i * 5
            )
        
        # Check stats were recorded
        self.assertIn(model_name, self.service.inference_stats)
        stats = self.service.inference_stats[model_name]
        
        self.assertEqual(len(stats), 5)
        for stat in stats:
            self.assertIn('inference_time', stat)
            self.assertIn('tokens_per_second', stat)
            self.assertIn('input_length', stat)
            self.assertIn('output_length', stat)
        
        print("‚úÖ Inference performance monitoring working")
        print(f"   Recorded {len(stats)} inference measurements")
    
    def test_performance_analytics(self):
        """Test performance analytics generation"""
        model_name = "test-model"
        
        # Add some test data
        for i in range(3):
            self.service.monitor_inference_performance(
                model_name=model_name,
                inference_time=0.2,
                input_length=100,
                output_length=50
            )
        
        # Get analytics
        analytics = self.service.get_performance_analytics(model_name)
        
        self.assertIn('model_name', analytics)
        self.assertIn('total_inferences', analytics)
        self.assertIn('recent_performance', analytics)
        
        recent = analytics['recent_performance']
        self.assertIn('avg_inference_time', recent)
        self.assertIn('avg_tokens_per_second', recent)
        
        print("‚úÖ Performance analytics working")
        print(f"   Total inferences: {analytics['total_inferences']}")
        print(f"   Avg inference time: {recent['avg_inference_time']:.4f}s")
    
    def test_cache_statistics(self):
        """Test cache statistics generation"""
        # Add some test data
        self.service.model_cache["test1"] = (Mock(), Mock())
        self.service.model_cache["test2"] = (Mock(), Mock())
        self.service._record_cache_hit("test-model", 0.1)
        
        stats = self.service.get_cache_statistics()
        
        self.assertIn('cache_capacity', stats)
        self.assertIn('memory_usage', stats)
        self.assertIn('performance_stats', stats)
        self.assertIn('active_optimizations', stats)
        
        print("‚úÖ Cache statistics working")
        print(f"   Cache capacity: {stats['cache_capacity']}")

class TestModelOptimizationCleanup(unittest.TestCase):
    """Test cleanup and maintenance features"""
    
    def setUp(self):
        if not MODEL_OPTIMIZATION_AVAILABLE:
            self.skipTest("Model optimization service not available")
        self.service = ModelOptimizationService()
    
    def test_cache_cleanup(self):
        """Test cache cleanup functionality"""
        # Add some test items to cache
        self.service.model_cache["old_item"] = (Mock(), Mock())
        self.service.tokenizer_cache["old_tokenizer"] = Mock()
        
        # Add metadata with old timestamp
        import time
        old_time = time.time() - (25 * 3600)  # 25 hours ago
        self.service.cache_metadata["old_item"] = {
            "model_name": "old-model",
            "config": {},
            "created": old_time,
            "last_access": old_time
        }
        
        # Run cleanup
        result = self.service.cleanup_cache(force=True)
        
        self.assertIsInstance(result, dict)
        self.assertIn('cleanup_completed', result)
        self.assertIn('items_cleaned', result)
        self.assertIn('cleanup_time', result)
        
        self.assertTrue(result['cleanup_completed'])
        
        print("‚úÖ Cache cleanup working")
        print(f"   Items cleaned: {result['items_cleaned']}")
        print(f"   Cleanup time: {result['cleanup_time']:.2f}s")

def run_all_tests():
    """Run all model optimization tests"""
    print("üß™ Running Model Optimization Service Tests")
    print("=" * 60)
    
    if not MODEL_OPTIMIZATION_AVAILABLE:
        print("‚ùå Model optimization service not available - cannot run tests")
        return False
    
    # Create test suite
    test_classes = [
        TestModelOptimizationDependencies,
        TestModelOptimizationMemoryManagement,
        TestModelOptimizationCaching,
        TestModelOptimizationFeatures,
        TestModelOptimizationPerformance,
        TestModelOptimizationCleanup
    ]
    
    suite = unittest.TestSuite()
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        suite.addTests(tests)
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    print("=" * 60)
    print(f"üìä Model Optimization Test Results:")
    print(f"   Tests run: {result.testsRun}")
    print(f"   Failures: {len(result.failures)}")
    print(f"   Errors: {len(result.errors)}")
    print(f"   Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")
    
    return len(result.failures) == 0 and len(result.errors) == 0

if __name__ == "__main__":
    success = run_all_tests()
    
    # Additional dependency check
    print("\nüîç Optimization Features Status:")
    print(f"   PyTorch: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'}")
    print(f"   Transformers: {'‚úÖ' if TRANSFORMERS_AVAILABLE else '‚ùå'}")
    print(f"   Bitsandbytes (quantization): {'‚úÖ' if BITSANDBYTES_AVAILABLE else '‚ùå'}")
    print(f"   Accelerate (device mapping): {'‚úÖ' if ACCELERATE_AVAILABLE else '‚ùå'}")
    print(f"   Optimum (BetterTransformer): {'‚úÖ' if OPTIMUM_AVAILABLE else '‚ùå'}")
    print(f"   Psutil (system monitoring): {'‚úÖ' if PSUTIL_AVAILABLE else '‚ùå'}")
    
    missing = []
    if not BITSANDBYTES_AVAILABLE:
        missing.append("pip install bitsandbytes  # For 4-bit quantization")
    if not ACCELERATE_AVAILABLE:
        missing.append("pip install accelerate  # For device mapping")
    if not OPTIMUM_AVAILABLE:
        missing.append("pip install optimum  # For BetterTransformer")
    if not PSUTIL_AVAILABLE:
        missing.append("pip install psutil  # For system monitoring")
    
    if missing:
        print("\n   To enable additional optimizations:")
        for cmd in missing:
            print(f"     {cmd}")
    
    print(f"\n{'üéâ All tests passed!' if success else '‚ö†Ô∏è  Some tests failed'}")
    sys.exit(0 if success else 1)