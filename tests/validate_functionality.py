"""
Comprehensive Functionality Validation Script
Tests all core features including STT confidence scoring preservation
"""
import sys
import os
import logging
import traceback
from pathlib import Path

# Add src to path for imports
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

def test_imports():
    """Test that all core imports work correctly"""
    print("Testing imports...")
    try:
        # Core models
        from src.models.core import (
            STTResult, TTSResult, VoiceProfile, ConversationTurn,
            ConfidenceLevel, QualityMetrics
        )
        print("âœ“ Core models imported successfully")
        
        # Services
        from src.services.stt_service import STTService
        from src.services.tts_service import TTSService
        from src.services.ai_service import AIService
        from src.services.voice_manager import VoiceManager
        print("âœ“ Services imported successfully")
        
        # Configuration
        from src.config.settings import get_config, get_config_manager
        print("âœ“ Configuration imported successfully")
        
        # Utilities
        from src.utils.dependencies import initialize_all, get_available_providers
        from src.utils.audio_utils import AudioProcessor
        from src.utils.file_utils import FileManager
        from src.utils.validation import Validator
        print("âœ“ Utilities imported successfully")
        
        # Main application
        from src.main import ChatbotApplication
        print("âœ“ Main application imported successfully")
        
        return True
    
    except Exception as e:
        print(f"âœ— Import failed: {e}")
        traceback.print_exc()
        return False

def test_core_models():
    """Test core model functionality"""
    print("\nTesting core models...")
    try:
        from src.models.core import (
            STTResult, TTSResult, VoiceProfile, ConversationTurn,
            ConfidenceLevel, QualityMetrics
        )
        
        # Test STT Result with confidence scoring
        quality = QualityMetrics(clarity=0.9, noise_level=0.1, volume=0.8)
        stt_result = STTResult(
            text="Test transcription",
            confidence=0.85,
            language="en",
            processing_time=1.5,
            model_used="test_model",
            quality_metrics=quality
        )
        
        # Test confidence level detection
        assert stt_result.is_high_confidence(0.8) == True
        assert stt_result.get_confidence_level(0.8, 0.6) == ConfidenceLevel.HIGH
        
        # Test quality score calculation
        quality_score = stt_result.quality_score()
        assert 0.0 <= quality_score <= 1.0
        
        print("âœ“ STT Result confidence scoring works correctly")
        
        # Test TTS Result
        tts_result = TTSResult(
            success=True,
            audio_path="/tmp/test.wav",
            duration=3.0,
            processing_time=2.0,
            voice_used="test_voice",
            quality_metrics=quality
        )
        
        assert tts_result.success == True
        print("âœ“ TTS Result creation works correctly")
        
        # Test Voice Profile
        voice_profile = VoiceProfile(
            model_name="test_model",
            speaker_name="test_speaker",
            gender="female",
            speed=1.2,
            volume=0.9
        )
        
        assert voice_profile.speed == 1.2
        print("âœ“ Voice Profile creation works correctly")
        
        # Test Conversation Turn
        turn = ConversationTurn(
            user_input=stt_result,
            ai_response="Test AI response",
            voice_output=tts_result
        )
        
        assert turn.overall_quality > 0.0
        feedback = turn.generate_feedback()
        assert "confidence_level" in feedback
        assert "suggestions" in feedback
        
        print("âœ“ Conversation Turn with feedback works correctly")
        
        return True
        
    except Exception as e:
        print(f"âœ— Core models test failed: {e}")
        traceback.print_exc()
        return False

def test_configuration():
    """Test configuration management"""
    print("\nTesting configuration...")
    try:
        from src.config.settings import get_config, get_config_manager
        
        # Get configuration manager
        config_manager = get_config_manager()
        config = config_manager.get_config()
        
        # Test STT configuration
        assert hasattr(config.stt, 'provider')
        assert hasattr(config.stt, 'high_confidence')
        assert hasattr(config.stt, 'medium_confidence')
        assert hasattr(config.stt, 'low_confidence')
        
        print("âœ“ STT configuration structure correct")
        
        # Test TTS configuration
        assert hasattr(config.tts, 'provider')
        assert hasattr(config.tts, 'speed')
        assert hasattr(config.tts, 'volume')
        
        print("âœ“ TTS configuration structure correct")
        
        # Test AI configuration
        assert hasattr(config.ai, 'model_name')
        assert hasattr(config.ai, 'temperature')
        assert hasattr(config.ai, 'max_length')
        
        print("âœ“ AI configuration structure correct")
        
        # Test configuration validation
        validation = config_manager.validate_config()
        print(f"âœ“ Configuration validation: {validation['valid']}")
        
        return True
        
    except Exception as e:
        print(f"âœ— Configuration test failed: {e}")
        traceback.print_exc()
        return False

def test_dependencies():
    """Test dependency management"""
    print("\nTesting dependencies...")
    try:
        from src.utils.dependencies import initialize_all, get_available_providers
        
        # Initialize dependencies
        initialize_all()
        
        # Get available providers
        providers = get_available_providers()
        
        assert "stt" in providers
        assert "tts" in providers
        assert "ai" in providers
        
        print("âœ“ Dependency initialization works correctly")
        print(f"âœ“ Available providers detected: {providers}")
        
        return True
        
    except Exception as e:
        print(f"âœ— Dependencies test failed: {e}")
        traceback.print_exc()
        return False

def test_services():
    """Test service initialization"""
    print("\nTesting services...")
    try:
        from src.services.stt_service import STTService
        from src.services.tts_service import TTSService
        from src.services.ai_service import AIService
        from src.services.voice_manager import VoiceManager
        from src.models.core import VoiceProfile
        
        # Test STT service initialization
        stt_service = STTService(provider="faster_whisper")
        stt_info = stt_service.get_model_info()
        assert "provider" in stt_info
        print("âœ“ STT Service initializes correctly")
        
        # Test TTS service initialization
        voice_profile = VoiceProfile()
        tts_service = TTSService(provider="coqui", voice_profile=voice_profile)
        tts_info = tts_service.get_model_info()
        assert "provider" in tts_info
        print("âœ“ TTS Service initializes correctly")
        
        # Test AI service initialization
        ai_service = AIService()
        ai_info = ai_service.get_model_info()
        assert "model_name" in ai_info
        print("âœ“ AI Service initializes correctly")
        
        # Test Voice Manager initialization
        voice_manager = VoiceManager()
        vm_info = voice_manager.get_service_info()
        assert "stt" in vm_info
        assert "tts" in vm_info
        assert "confidence_thresholds" in vm_info
        print("âœ“ Voice Manager initializes correctly")
        
        return True
        
    except Exception as e:
        print(f"âœ— Services test failed: {e}")
        traceback.print_exc()
        return False

def test_confidence_scoring():
    """Test detailed confidence scoring functionality"""
    print("\nTesting confidence scoring...")
    try:
        from src.models.core import STTResult, QualityMetrics, ConfidenceLevel
        
        # Test high confidence scenario
        high_conf_result = STTResult(
            text="This is a clear transcription",
            confidence=0.95,
            language="en",
            processing_time=1.0,
            model_used="test",
            quality_metrics=QualityMetrics(clarity=0.9, noise_level=0.1, volume=0.8)
        )
        
        assert high_conf_result.is_high_confidence(0.8) == True
        assert high_conf_result.is_acceptable(0.6) == True
        assert high_conf_result.get_confidence_level(0.8, 0.6) == ConfidenceLevel.HIGH
        
        quality_score = high_conf_result.quality_score()
        assert quality_score > 0.8
        
        print("âœ“ High confidence scoring works correctly")
        
        # Test medium confidence scenario
        med_conf_result = STTResult(
            text="Okay transcription",
            confidence=0.7,
            language="en",
            processing_time=2.0,
            model_used="test",
            quality_metrics=QualityMetrics(clarity=0.7, noise_level=0.3, volume=0.6)
        )
        
        assert med_conf_result.is_high_confidence(0.8) == False
        assert med_conf_result.is_acceptable(0.6) == True
        assert med_conf_result.get_confidence_level(0.8, 0.6) == ConfidenceLevel.MEDIUM
        
        print("âœ“ Medium confidence scoring works correctly")
        
        # Test low confidence scenario
        low_conf_result = STTResult(
            text="Poor transcription",
            confidence=0.4,
            language="en",
            processing_time=3.0,
            model_used="test",
            quality_metrics=QualityMetrics(clarity=0.4, noise_level=0.7, volume=0.3)
        )
        
        assert low_conf_result.is_high_confidence(0.8) == False
        assert low_conf_result.is_acceptable(0.6) == False
        assert low_conf_result.get_confidence_level(0.8, 0.6) == ConfidenceLevel.LOW
        
        quality_score_low = low_conf_result.quality_score()
        assert quality_score_low < 0.6
        
        print("âœ“ Low confidence scoring works correctly")
        
        return True
        
    except Exception as e:
        print(f"âœ— Confidence scoring test failed: {e}")
        traceback.print_exc()
        return False

def test_main_application():
    """Test main application integration"""
    print("\nTesting main application...")
    try:
        from src.main import ChatbotApplication
        
        # Initialize application (this may fail due to missing models, but should not crash)
        try:
            app = ChatbotApplication()
            print("âœ“ Application initializes successfully")
            
            # Test service info retrieval
            analysis = app.get_session_analysis()
            assert isinstance(analysis, dict)
            print("âœ“ Session analysis works")
            
            # Test configuration access
            assert app.config is not None
            print("âœ“ Configuration access works")
            
            # Test shutdown
            app.shutdown()
            print("âœ“ Application shutdown works")
            
        except Exception as init_error:
            # Expected if models aren't available - this is okay
            print(f"âš  Application initialization had expected issues: {init_error}")
            print("  This is normal if ML models aren't installed")
        
        return True
        
    except Exception as e:
        print(f"âœ— Main application test failed: {e}")
        traceback.print_exc()
        return False

def test_utilities():
    """Test utility functions"""
    print("\nTesting utilities...")
    try:
        from src.utils.validation import Validator
        from src.utils.file_utils import FileManager
        from src.utils.audio_utils import AudioProcessor
        
        # Test validation
        text_validation = Validator.validate_text_input("Hello world", min_length=5)
        assert text_validation["valid"] == True
        print("âœ“ Text validation works")
        
        confidence_validation = Validator.validate_confidence_score(0.8)
        assert confidence_validation["valid"] == True
        assert confidence_validation["level"] == "high"
        print("âœ“ Confidence validation works")
        
        # Test file manager
        file_manager = FileManager()
        temp_file = file_manager.create_temp_file(content="test")
        assert temp_file.exists()
        file_manager.cleanup_temp_files()
        print("âœ“ File management works")
        
        # Test audio processor basic functionality
        supported_formats = AudioProcessor.SUPPORTED_FORMATS
        assert '.wav' in supported_formats
        print("âœ“ Audio processor structure correct")
        
        return True
        
    except Exception as e:
        print(f"âœ— Utilities test failed: {e}")
        traceback.print_exc()
        return False

def main():
    """Run comprehensive validation"""
    print("=" * 60)
    print("COMPREHENSIVE FUNCTIONALITY VALIDATION")
    print("=" * 60)
    
    tests = [
        ("Imports", test_imports),
        ("Core Models", test_core_models),
        ("Configuration", test_configuration),
        ("Dependencies", test_dependencies),
        ("Services", test_services),
        ("Confidence Scoring", test_confidence_scoring),
        ("Main Application", test_main_application),
        ("Utilities", test_utilities)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"\nFATAL ERROR in {test_name}: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 60)
    print("VALIDATION SUMMARY")
    print("=" * 60)
    
    passed = 0
    total = len(tests)
    
    for test_name, result in results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        print(f"{test_name:20} {status}")
        if result:
            passed += 1
    
    print(f"\nResults: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nðŸŽ‰ ALL FUNCTIONALITY VALIDATION TESTS PASSED!")
        print("âœ… STT confidence scoring is preserved")
        print("âœ… TTS quality metrics are maintained")
        print("âœ… AI services are functional")
        print("âœ… Configuration management works")
        print("âœ… Core models and services are operational")
    else:
        print(f"\nâš ï¸  {total - passed} tests failed - review above for details")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)